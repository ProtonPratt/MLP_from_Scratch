{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' MLP class  '''\n",
    "\n",
    "class MLP:\n",
    "    ''' Multi-layer perceptron class '''\n",
    "    ''' input_size, hidden_layers = [sizes of the hidden layers], output_size, epochs, learning_rate, batch_size, activation_function, loss_function '''\n",
    "    def __init__(self, input_size, hidden_layers, output_size, epochs = 100, learning_rate = 0.01, early_stopping = False,\n",
    "                 batch_size = 32, activation_function = 'relu', loss_function = 'mse', optimizer = 'sgd'):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_size = output_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.early_stopping = early_stopping\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.history = []\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        ''' Initialize weights '''\n",
    "        layers = [self.input_size] + self.hidden_layers + [self.output_size]\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.weights.append(np.random.randn(layers[i], layers[i+1])) # Check the weights initialization\n",
    "            self.biases.append(np.zeros(layers[i+1]))\n",
    "    \n",
    "    # Activation functions       \n",
    "    def relu(self, x):\n",
    "        ''' ReLU activation function '''\n",
    "        return np.maximum(0, x)\n",
    "    def relu_derivative(self, x):\n",
    "        ''' ReLU derivative '''\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        ''' Sigmoid activation function '''\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def sigmoid_derivative(self, x):\n",
    "        ''' Sigmoid derivative '''\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        ''' Tanh activation function '''\n",
    "        return np.tanh(x)\n",
    "    def tanh_derivative(self, x):\n",
    "        ''' Tanh derivative '''\n",
    "        return 1 - np.tanh(x)**2\n",
    "    \n",
    "    def linear(self, x):\n",
    "        ''' Linear activation function '''\n",
    "        return x\n",
    "    def linear_derivative(self, x):\n",
    "        ''' Linear derivative '''\n",
    "        return 1\n",
    "            \n",
    "    def _activation(self):\n",
    "        ''' Activation function '''\n",
    "        if self.activation_function == 'relu':\n",
    "            return self.relu, self.relu_derivative\n",
    "        elif self.activation_function == 'sigmoid':\n",
    "            return self.sigmoid, self.sigmoid_derivative\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return self.tanh, self.tanh_derivative\n",
    "        elif self.activation_function == 'linear':\n",
    "            return self.linear, self.linear_derivative\n",
    "        else:\n",
    "            raise ValueError('Activation function not supported')\n",
    "    \n",
    "    # Optimizers\n",
    "    def mse(self, y, y_pred):\n",
    "        ''' Mean squared error '''\n",
    "        return np.mean((y - y_pred)**2)\n",
    "    def mse_derivative(self, y, y_pred):\n",
    "        ''' Mean squared error derivative '''\n",
    "        return y_pred - y\n",
    "    \n",
    "    def cross_entropy(self, y, y_pred):\n",
    "        ''' Cross entropy loss '''\n",
    "        return -np.sum(y * np.log(y_pred))\n",
    "    def cross_entropy_derivative(self, y, y_pred):\n",
    "        ''' Cross entropy derivative '''\n",
    "        return (y_pred - y) / (y_pred * (1 - y_pred))\n",
    "    \n",
    "    def _loss(self):\n",
    "        ''' Loss function '''\n",
    "        if self.loss_function == 'mse':\n",
    "            return self.mse, self.mse_derivative\n",
    "        elif self.loss_function == 'cross_entropy':\n",
    "            return self.cross_entropy, self.cross_entropy_derivative\n",
    "        else:\n",
    "            raise ValueError('Loss function not supported')\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, X):\n",
    "        ''' Forward pass '''\n",
    "        activations = []\n",
    "        Z = [X]\n",
    "        \n",
    "        activation, _ = self._activation()\n",
    "        current_activation = X\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(current_activation, self.weights[i]) + self.biases[i]\n",
    "            Z.append(z)\n",
    "            current_activation = activation(z)\n",
    "            activations.append(current_activation)\n",
    "            \n",
    "        # Output layer\n",
    "        z = np.dot(current_activation, self.weights[-1]) + self.biases[-1]\n",
    "        Z.append(z)\n",
    "        output_activation = activation(z)\n",
    "        activations.append(output_activation)\n",
    "        \n",
    "        return Z, activations\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, X, y, Z, activations):\n",
    "        ''' Backward pass '''\n",
    "        grads = {}\n",
    "        m = y.shape[0]\n",
    "        # Activation function\n",
    "        activation_function, activation_derivative = self._activation()\n",
    "        \n",
    "        # Loss function\n",
    "        loss, loss_derivative = self._loss()\n",
    "        \n",
    "        # backprop loss\n",
    "        dA = loss_derivative(y, activations[-1])\n",
    "        dZ = dA * activation_derivative(Z[-1])\n",
    "        grads[\"dW\" + str(len(self.weights)-1)] = np.dot(activations[-2].T, dZ) / m\n",
    "        grads[\"db\" + str(len(self.weights)-1)] = np.sum(dZ, axis=0) / m\n",
    "        \n",
    "        # backprop hidden layers\n",
    "        for i in range(len(self.weights)-2, -1, -1):\n",
    "            dA = np.dot(dZ, self.weights[i+1].T)\n",
    "            dZ = dA * activation_derivative(Z[i+1])\n",
    "            grads[\"dW\" + str(i)] = np.dot(activations[i].T, dZ) / m\n",
    "            grads[\"db\" + str(i)] = np.sum(dZ, axis=0) / m\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    # Update weights\n",
    "    def update_weights(self, grads):\n",
    "        ''' Update weights '''\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * grads[\"dW\" + str(i)]\n",
    "            self.biases[i] -= self.learning_rate * grads[\"db\" + str(i)]\n",
    "            \n",
    "    # Train the model\n",
    "    def fit(self, X, y):\n",
    "        ''' Train the model '''\n",
    "        for epoch in range(self.epochs):\n",
    "            self.optimize(X, y)\n",
    "            \n",
    "            if self.early_stopping:\n",
    "                pass\n",
    "            \n",
    "    # Optimize\n",
    "    def optimize(self, X, y):\n",
    "        ''' Optimize the model '''\n",
    "        if self.optimizer == 'sgd':\n",
    "            self.sgd(X, y)\n",
    "        elif self.optimizer == 'mini_batch':\n",
    "            self.mini_batch(X, y)\n",
    "        elif self.optimizer == 'full_batch':\n",
    "            self.batch(X, y)\n",
    "        else:\n",
    "            raise ValueError('Optimizer not supported')\n",
    "    \n",
    "    def sgd(self, X, y):\n",
    "        ''' Stochastic gradient descent '''\n",
    "        for i in range(len(X[0])):\n",
    "            Z, activations = self.forward(X[i])\n",
    "            grads = self.backward(X[i], y[i], Z, activations)\n",
    "            self.update_weights(grads)\n",
    "            \n",
    "    def mini_batch(self, X, y):\n",
    "        ''' Mini-batch gradient descent '''\n",
    "        for i in range(0, X.shape[0], self.batch_size):\n",
    "            Z, activations = self.forward(X[i:i+self.batch_size])\n",
    "            grads = self.backward(X[i:i+self.batch_size], y[i:i+self.batch_size], Z, activations)\n",
    "            self.update_weights(grads)\n",
    "            \n",
    "    def batch(self, X, y):\n",
    "        ''' Batch gradient descent '''\n",
    "        Z, activations = self.forward(X)\n",
    "        grads = self.backward(X, y, Z, activations)\n",
    "        self.update_weights(grads)\n",
    "        \n",
    "    # Predict\n",
    "    def predict(self, X):\n",
    "        ''' Predict '''\n",
    "        Z, activations = self.forward(X)\n",
    "        return activations[-1]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,3) (3,3) (2,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(X[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(mlp\u001b[38;5;241m.\u001b[39mpredict(X))\n",
      "Cell \u001b[1;32mIn[3], line 160\u001b[0m, in \u001b[0;36mMLP.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' Train the model '''\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping:\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 173\u001b[0m, in \u001b[0;36mMLP.optimize\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmini_batch(X, y)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_batch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptimizer not supported\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 195\u001b[0m, in \u001b[0;36mMLP.batch\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    193\u001b[0m Z, activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[0;32m    194\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward(X, y, Z, activations)\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 153\u001b[0m, in \u001b[0;36mMLP.update_weights\u001b[1;34m(self, grads)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m''' Update weights '''\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)):\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdW\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m*\u001b[39m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)]\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,3) (3,3) (2,3) "
     ]
    }
   ],
   "source": [
    "# Test the MLP\n",
    "mlp = MLP(2, [3, 3], 1, epochs = 100, learning_rate = 0.01, batch_size = 32, activation_function = 'relu', loss_function = 'mse', optimizer = 'full_batch')\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "print(X[0])\n",
    "\n",
    "mlp.fit(X, y)\n",
    "print(mlp.predict(X))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
