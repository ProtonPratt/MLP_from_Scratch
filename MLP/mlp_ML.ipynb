{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' MLP class  '''\n",
    "\n",
    "class MLP:\n",
    "    ''' Multi-layer perceptron class '''\n",
    "    ''' input_size, hidden_layers = [sizes of the hidden layers], output_size, epochs, learning_rate, batch_size, activation_function, loss_function '''\n",
    "    def __init__(self, input_size, hidden_layers, output_size, epochs = 100, learning_rate = 0.01, early_stopping = False,\n",
    "                 batch_size = 32, activation_function = 'relu', loss_function = 'mse', optimizer = 'sgd', beta = 0.0, \n",
    "                 Random_state = None, weight_init = 'he', model_type = 'class_ML', wandb_vaar = False, run_start = \"hyperparam_tuning\"):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_size = output_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.early_stopping = early_stopping\n",
    "        self.beta = beta\n",
    "        self.weight_init = weight_init\n",
    "        self.model_type = model_type\n",
    "        self.wandb_vaar = wandb_vaar\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.loss = []\n",
    "        self.history = []\n",
    "        \n",
    "        np.random.seed(Random_state)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        \n",
    "        run_name = f\"{run_start}-{self.activation_function}-{self.optimizer}-{self.loss_function}-{self.weight_init}-{self.epochs}-{self.learning_rate}-{self.batch_size}\"\n",
    "        \n",
    "        # Initialize WandB\n",
    "        if wandb_vaar:\n",
    "            wandb.init(\n",
    "                project=\"SMAI_A3-MLP_MultiLable\", \n",
    "                name=run_name,\n",
    "                config={\n",
    "                \"learning_rate\": self.learning_rate,\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"activation\": self.activation_function,\n",
    "                \"optimizer\": self.optimizer,\n",
    "                \"input_size\": self.input_size,\n",
    "                \"hidden_layer_sizes\": self.hidden_layers,\n",
    "                \"output_size\": self.output_size,\n",
    "                \"loss_function\": self.loss_function,\n",
    "                \"weight_init\": self.weight_init,\n",
    "                \"Random_state\": Random_state,\n",
    "                \"beta\": self.beta,\n",
    "                \"early_stopping\": self.early_stopping,\n",
    "            })\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        ''' Initialize weights '''\n",
    "        layers = [self.input_size] + self.hidden_layers + [self.output_size]\n",
    "        for i in range(len(layers) - 1):\n",
    "            if self.weight_init == 'random':\n",
    "                self.weights.append(np.random.randn(layers[i], layers[i+1]))  # Random initialization\n",
    "            \n",
    "            elif self.weight_init == 'he':\n",
    "                self.weights.append(np.random.randn(layers[i], layers[i+1]) * np.sqrt(2. / layers[i]))  # He initialization\n",
    "            \n",
    "            elif self.weight_init == 'xavier':\n",
    "                limit = np.sqrt(6 / (layers[i] + layers[i+1]))\n",
    "                self.weights.append(np.random.uniform(-limit, limit, (layers[i], layers[i+1])))  # Xavier/Glorot initialization\n",
    "            \n",
    "            self.biases.append(np.zeros(layers[i+1]))\n",
    "        \n",
    "        self.vW = [np.zeros_like(w) for w in self.weights]  # Initialize velocity for weights\n",
    "        self.vb = [np.zeros_like(b) for b in self.biases]   # Initialize velocity for biases\n",
    "    \n",
    "    # Activation functions       \n",
    "    def relu(self, x):\n",
    "        ''' ReLU activation function '''\n",
    "        return np.where(x > 0, x, x * 0.01)\n",
    "    def relu_derivative(self, x):\n",
    "        ''' ReLU derivative '''\n",
    "        return np.where(x > 0, 1, 0.01)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        ''' Sigmoid activation function '''\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def sigmoid_derivative(self, x):\n",
    "        ''' Sigmoid derivative '''\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        ''' Tanh activation function '''\n",
    "        return np.tanh(x)\n",
    "    def tanh_derivative(self, x):\n",
    "        ''' Tanh derivative '''\n",
    "        return 1 - np.tanh(x)**2\n",
    "    \n",
    "    def linear(self, x):\n",
    "        ''' Linear activation function '''\n",
    "        return x\n",
    "    def linear_derivative(self, x):\n",
    "        ''' Linear derivative '''\n",
    "        return 1\n",
    "            \n",
    "    def _activation(self, activation_function = None):\n",
    "        ''' Activation function '''\n",
    "        if activation_function == 'relu':\n",
    "            return self.relu, self.relu_derivative\n",
    "        elif activation_function == 'sigmoid':\n",
    "            return self.sigmoid, self.sigmoid_derivative\n",
    "        elif activation_function == 'tanh':\n",
    "            return self.tanh, self.tanh_derivative\n",
    "        elif activation_function == 'linear':\n",
    "            return self.linear, self.linear_derivative\n",
    "        else:\n",
    "            raise ValueError('Activation function not supported')\n",
    "    \n",
    "    # Optimizers\n",
    "    def mse(self, y, y_pred):\n",
    "        ''' Mean squared error '''\n",
    "        return np.mean((y - y_pred)**2)\n",
    "    def mse_derivative(self, y, y_pred):\n",
    "        ''' Mean squared error derivative '''\n",
    "        return 2*(y_pred - y)\n",
    "    \n",
    "    def cross_entropy(self, y, y_pred):\n",
    "        ''' Cross entropy loss '''\n",
    "        return -np.sum(y * np.log(y_pred)) / len(y)\n",
    "    \n",
    "    def cross_entropy_derivative(self, y, y_pred):\n",
    "        ''' Cross entropy derivative '''\n",
    "        return (y_pred - y) # / (y_pred * (1 - y_pred) + 1e-6)  \n",
    "    \n",
    "    def _loss(self):\n",
    "        ''' Loss function '''\n",
    "        if self.loss_function == 'mse':\n",
    "            return self.mse, self.mse_derivative\n",
    "        elif self.loss_function == 'cross_entropy':\n",
    "            return self.cross_entropy, self.cross_entropy_derivative\n",
    "        else:\n",
    "            raise ValueError('Loss function not supported')\n",
    "        \n",
    "    def _softmax(self, x):\n",
    "        ''' Softmax activation function '''\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    def _one_hot(self, y):\n",
    "        ''' One-hot encoding '''\n",
    "        print(y.min(), y.max())\n",
    "        one_hot = np.zeros((y.size, y.max()+1- y.min()))\n",
    "        for i in range(y.size):\n",
    "            one_hot[i, y[i] - y.min()] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def filter_onehot(self, y_true, y_test):\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "        return y_true, y_test\n",
    "    \n",
    "    def filter_softmax(self, y_pred):\n",
    "        # if >0.5, then 1 else 0\n",
    "        return np.where(y_pred > 0.5, 1, 0)\n",
    "        \n",
    "    \n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct_predictions = np.sum(y_true == y_pred)\n",
    "        total_predictions = len(y_true)\n",
    "        return correct_predictions / total_predictions\n",
    "    \n",
    "    def hamming_loss(self, y_true, y_pred):\n",
    "        # Compute the number of incorrectly predicted labels\n",
    "        incorrect_labels = np.sum(y_true != y_pred)\n",
    "        \n",
    "        # Total number of labels is the number of samples times the number of labels per sample\n",
    "        total_labels = y_true.shape[0] * y_true.shape[1]\n",
    "        \n",
    "        # Hamming Loss: Proportion of incorrect labels\n",
    "        return incorrect_labels / total_labels\n",
    "    \n",
    "    def multi_label_partial_accuracy(self, y_true, y_pred):\n",
    "        y_pred_partial = y_pred.reshape(-1, 1)\n",
    "        y_true_partial = y_true.reshape(-1, 1)\n",
    "        partial_accuracy = self.accuracy(y_true_partial, y_pred_partial)\n",
    "        return partial_accuracy\n",
    "    \n",
    "    def multi_label_accuracy(y_true, y_pred):\n",
    "        # Exact match accuracy (where all labels for a sample are correctly predicted)\n",
    "        correct_predictions = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "        total_predictions = len(y_true)\n",
    "        return correct_predictions / total_predictions\n",
    "    \n",
    "    def multi_label_confusion_matrix(self, y_true, y_pred):\n",
    "        num_labels = y_true.shape[1]\n",
    "        confusion_matrices = []\n",
    "\n",
    "        for i in range(num_labels):\n",
    "            tp = np.sum((y_pred[:, i] == 1) & (y_true[:, i] == 1))  # True Positive\n",
    "            tn = np.sum((y_pred[:, i] == 0) & (y_true[:, i] == 0))  # True Negative\n",
    "            fp = np.sum((y_pred[:, i] == 1) & (y_true[:, i] == 0))  # False Positive\n",
    "            fn = np.sum((y_pred[:, i] == 0) & (y_true[:, i] == 1))  # False Negative\n",
    "\n",
    "            matrix = np.array([[tn, fp], [fn, tp]])\n",
    "            confusion_matrices.append(matrix)\n",
    "\n",
    "        return np.array(confusion_matrices)\n",
    "    \n",
    "    def multi_label_micro_metrics_partial(self, y_true, y_pred):\n",
    "        # confusion matrix\n",
    "        confusion_matrix = self.multi_label_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        # micro-averaged precision, recall, f1_score\n",
    "        micro_precision = np.sum(confusion_matrix[:, 0, 0]) / np.sum(confusion_matrix[:, 0, 0] + np.sum(confusion_matrix[:, 1, 0]))\n",
    "        micro_recall = np.sum(confusion_matrix[:, 0, 0]) / np.sum(confusion_matrix[:, 0, 0] + np.sum(confusion_matrix[:, 1, 1]))\n",
    "        micro_f1_score = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall)\n",
    "        \n",
    "        return micro_precision, micro_recall, micro_f1_score\n",
    "    \n",
    "    def multi_label_macro_metrics_partial(self, y_true, y_pred):\n",
    "        # confusion matrix\n",
    "        confusion_matrix = self.multi_label_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        # macro-averaged precision, recall, f1_score\n",
    "        macro_precision = np.mean(confusion_matrix[:, 0, 0] / (confusion_matrix[:, 0, 0] + confusion_matrix[:, 1, 0]))\n",
    "        macro_recall = np.mean(confusion_matrix[:, 0, 0] / (confusion_matrix[:, 0, 0] + confusion_matrix[:, 1, 1]))\n",
    "        macro_f1_score = 2 * (macro_precision * macro_recall) / (macro_precision + macro_recall)\n",
    "        \n",
    "        return macro_precision, macro_recall, macro_f1_score\n",
    "    \n",
    "    def multi_label_precision_recall_f1(y_true, y_pred):\n",
    "        epsilon = 1e-9  # To avoid division by zero\n",
    "\n",
    "        # True Positives, False Positives, and False Negatives for each label\n",
    "        true_positives = np.sum((y_pred == 1) & (y_true == 1), axis=0)\n",
    "        false_positives = np.sum((y_pred == 1) & (y_true == 0), axis=0)\n",
    "        false_negatives = np.sum((y_pred == 0) & (y_true == 1), axis=0)\n",
    "\n",
    "        # Precision: TP / (TP + FP)\n",
    "        precision = true_positives / (true_positives + false_positives + epsilon)\n",
    "        \n",
    "        # Recall: TP / (TP + FN)\n",
    "        recall = true_positives / (true_positives + false_negatives + epsilon)\n",
    "        \n",
    "        # F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "\n",
    "        # Return macro-average (mean over all labels)\n",
    "        return precision, recall, f1\n",
    "    \n",
    "    def confusion_matrix(self, y_true, y_pred):\n",
    "        classes = np.unique(y_true)\n",
    "        matrix = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "\n",
    "        for i in range(len(y_true)):\n",
    "            true_idx = np.where(classes == y_true[i])[0][0]\n",
    "            pred_idx = np.where(classes == y_pred[i])[0][0]\n",
    "            matrix[true_idx, pred_idx] += 1\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def recall(self, y_true, y_pred):\n",
    "        matrix = self.confusion_matrix(y_true, y_pred)\n",
    "        recall_values = np.diag(matrix) / np.sum(matrix, axis=1)  # TP / (TP + FN)\n",
    "        recall_values = np.nan_to_num(recall_values)  # Handle division by zero\n",
    "        return np.mean(recall_values)\n",
    "    \n",
    "    def precision(self, y_true, y_pred):\n",
    "        matrix = self.confusion_matrix(y_true, y_pred)\n",
    "        precision_values = np.diag(matrix) / np.sum(matrix, axis=0)  # TP / (TP + FP)\n",
    "        precision_values = np.nan_to_num(precision_values)  # Handle division by zero\n",
    "        return np.mean(precision_values)\n",
    "    \n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        prec = self.precision(y_true, y_pred)\n",
    "        rec = self.recall(y_true, y_pred)\n",
    "        if (prec + rec) == 0:\n",
    "            return 0\n",
    "        return 2 * (prec * rec) / (prec + rec)\n",
    "    \n",
    "    def model_functions(self):\n",
    "        if self.model_type == 'class_ML':\n",
    "            return self.sigmoid, self.sigmoid_derivative\n",
    "        elif self.model_type == 'class_MC':\n",
    "            return self._softmax, self.cross_entropy_derivative\n",
    "        elif self.model_type == 'regression':\n",
    "            return self._activation(self.activation_function)\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, X):\n",
    "        ''' Forward pass '''\n",
    "        activations = [X]\n",
    "        Z = [X]\n",
    "        \n",
    "        activation, _ = self._activation(self.activation_function)\n",
    "        current_activation = X\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(current_activation, self.weights[i]) + self.biases[i]\n",
    "            Z.append(z)\n",
    "            current_activation = activation(z)\n",
    "            activations.append(current_activation)\n",
    "            \n",
    "        # Output layer\n",
    "        z = np.dot(current_activation, self.weights[-1]) + self.biases[-1]\n",
    "        Z.append(z)\n",
    "        \n",
    "        # Check if the model is for classification or regression\n",
    "        activation_function, _ = self.model_functions()\n",
    "        # output_activation = self.relu(z) # Output activation function\n",
    "        output_activation = activation_function(z) # Output activation function\n",
    "        activations.append(output_activation)\n",
    "        self.activations = activations\n",
    "        \n",
    "        return Z, activations\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, X, y, Z, activations):\n",
    "        ''' Backward pass '''\n",
    "        grads = {}\n",
    "        m = y.shape[0]\n",
    "        \n",
    "        # Activation function\n",
    "        # activation_function, activation_derivative = self._activation(self.activation_function)\n",
    "        activation_function, activation_derivative = self.model_functions()\n",
    "        \n",
    "        # Loss function\n",
    "        # loss, loss_derivative = self._loss()\n",
    "        if self.model_type == 'class_ML':\n",
    "            loss, loss_derivative = self.cross_entropy, self.cross_entropy_derivative\n",
    "        elif self.model_type == 'class_MC':\n",
    "            loss, loss_derivative = self.cross_entropy, self.cross_entropy_derivative\n",
    "        elif self.model_type == 'regression':\n",
    "            loss, loss_derivative = self.mse, self.mse_derivative\n",
    "        \n",
    "        # backprop loss in output layer\n",
    "        dA = loss_derivative(y, (activations[-1]))\n",
    "        if self.model_type == 'class_MC':\n",
    "            dZ = (activations[-1] - y) / m\n",
    "        else:\n",
    "            dZ = dA * activation_derivative(Z[-1])\n",
    "        \n",
    "        grads[\"dW\" + str(len(self.weights)-1)] = np.dot(activations[-2].T, dZ) / m\n",
    "        grads[\"db\" + str(len(self.weights)-1)] = np.sum(dZ, axis=0) / m\n",
    "        \n",
    "        # backprop hidden layers\n",
    "        activation_function, activation_derivative = self._activation(self.activation_function)\n",
    "        for i in range(len(self.weights)-2, -1, -1):\n",
    "            dA = np.dot(dZ, self.weights[i+1].T)\n",
    "            dZ = dA * activation_derivative(Z[i+1])\n",
    "            grads[\"dW\" + str(i)] = np.dot(activations[i].T, dZ) / m\n",
    "            grads[\"db\" + str(i)] = np.sum(dZ, axis=0) / m\n",
    "            \n",
    "        return grads\n",
    "    \n",
    "    # # Update weights\n",
    "    # def update_weights(self, grads):\n",
    "    #     ''' Update weights '''\n",
    "    #     # print('weights:', len(self.weights))\n",
    "    #     # for i in range(len(self.weights)):\n",
    "    #     #     print(self.weights[i].shape, grads[\"dW\" + str(i)].shape)\n",
    "    #     for i in range(len(self.weights)):\n",
    "    #         self.weights[i] -= self.learning_rate * grads[\"dW\" + str(i)]\n",
    "    #         self.biases[i] -= self.learning_rate * grads[\"db\" + str(i)]\n",
    "    \n",
    "    # update weights using momentum\n",
    "    def update_weights(self, grads):\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update the velocity for weights and biases using momentum\n",
    "            self.vW[i] = self.beta * self.vW[i] + (1 - self.beta) * grads[\"dW\" + str(i)]\n",
    "            self.vb[i] = self.beta * self.vb[i] + (1 - self.beta) * grads[\"db\" + str(i)]\n",
    "            \n",
    "            # Update the weights and biases using velocity\n",
    "            self.weights[i] -= self.learning_rate * self.vW[i]\n",
    "            self.biases[i] -= self.learning_rate * self.vb[i]\n",
    "            \n",
    "    # Train the model\n",
    "    def fit(self, X, y):\n",
    "        # encode the target variable if it is not regression\n",
    "        # if not self.model_type == 'regression':\n",
    "        #     y = self._one_hot(y)\n",
    "        \n",
    "        ''' Train the model '''\n",
    "        for epoch in range(self.epochs):\n",
    "            self.optimize(X, y)\n",
    "            \n",
    "            # Loss\n",
    "            loss_func, loss_derivative = self._loss()\n",
    "            y_pred = self.forward(X)[1][-1] # Predictions\n",
    "            self.loss.append(loss_func(y, y_pred))\n",
    "            \n",
    "            # Log accuracy, recall, precision, f1_score\n",
    "            if not self.model_type == 'regression' and self.wandb_vaar:\n",
    "                y_true, y_pred = self.filter_onehot(y, y_pred)\n",
    "                accuracy = self.accuracy(y_true, y_pred)\n",
    "                recall = self.recall(y_true, y_pred)\n",
    "                precision = self.precision(y_true, y_pred)\n",
    "                f1_score = self.f1_score(y_true, y_pred)\n",
    "                \n",
    "            \n",
    "            if self.wandb_vaar and self.wandb_vaar and not self.model_type == 'regression':\n",
    "                wandb.log({\"epoch\":epoch+1, \"loss\": self.loss[-1], \"accuracy\": accuracy, \"recall\": recall, \"precision\": precision, \"f1_score\": f1_score})\n",
    "            elif self.wandb_vaar:\n",
    "                wandb.log({\"epoch\":epoch+1, \"loss\": self.loss[-1]})\n",
    "            \n",
    "            if self.early_stopping:\n",
    "                if len(self.loss) > 1 and abs(self.loss[-1] - self.loss[-2]) < 1e-6:\n",
    "                    print(f\"Converged after {epoch+1} epochs\")\n",
    "                    break\n",
    "        \n",
    "        if self.wandb_vaar:\n",
    "            wandb.finish()\n",
    "            \n",
    "    # Optimize\n",
    "    def optimize(self, X, y):\n",
    "        ''' Optimize the model '''\n",
    "        if self.optimizer == 'sgd':\n",
    "            self.sgd(X, y)\n",
    "        elif self.optimizer == 'mini_batch':\n",
    "            self.mini_batch(X, y)\n",
    "        elif self.optimizer == 'full_batch':\n",
    "            self.batch(X, y)\n",
    "        else:\n",
    "            raise ValueError('Optimizer not supported')\n",
    "    \n",
    "    def sgd(self, X, y):\n",
    "        ''' Stochastic gradient descent '''\n",
    "        for i in range(X.shape[0]):\n",
    "            Z, activations = self.forward(X[i])\n",
    "            \n",
    "            # Adjust the sizes of matrix\n",
    "            for j in range(len(activations)):\n",
    "                activations[j] = activations[j].reshape(1, -1)\n",
    "                Z[j] = Z[j].reshape(1, -1)\n",
    "            \n",
    "            grads = self.backward(X[i], y[i].reshape(1,-1), Z, activations)\n",
    "            self.update_weights(grads)\n",
    "    \n",
    "    def mini_batch(self, X, y):\n",
    "        ''' Mini-batch gradient descent '''\n",
    "        for i in range(0, X.shape[0], self.batch_size):\n",
    "            Z, activations = self.forward(X[i:i+self.batch_size])\n",
    "            grads = self.backward(X[i:i+self.batch_size], y[i:i+self.batch_size], Z, activations)\n",
    "            self.update_weights(grads)\n",
    "            \n",
    "    def batch(self, X, y):\n",
    "        ''' Batch gradient descent '''\n",
    "        Z, activations = self.forward(X)\n",
    "        grads = self.backward(X, y, Z, activations)\n",
    "        self.update_weights(grads)\n",
    "        \n",
    "    # Predict\n",
    "    def predict(self, X):\n",
    "        ''' Predict '''\n",
    "        Z, activations = self.forward(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    # Evaluate\n",
    "    def evaluate(self, X, y):\n",
    "        ''' Evaluate '''\n",
    "        y_pred = self.predict(X)\n",
    "        loss, _ = self._loss()\n",
    "        return loss(y, y_pred)\n",
    "    \n",
    "    def _compute_loss(self, X, y):\n",
    "        ''' Compute loss '''\n",
    "        Z, activations = self.forward(X)\n",
    "        loss, _ = self._loss()\n",
    "        return loss(y, activations[-1])\n",
    "    \n",
    "    # gradient checks\n",
    "    def check_gradients(self, X, y):\n",
    "        ''' Gradient checks '''\n",
    "        epsilon = 1e-4\n",
    "        Z, activations = self.forward(X)\n",
    "        grads = self.backward(X, y, Z, activations)\n",
    "        \n",
    "        numerical_grads_list = []\n",
    "        analytical_grads_list = []\n",
    "        \n",
    "        # Check gradients\n",
    "        for i in range(len(self.weights)):\n",
    "            numerical_grads = np.zeros_like(self.weights[i])\n",
    "            for j in range(self.weights[i].shape[0]):\n",
    "                for k in range(self.weights[i].shape[1]):\n",
    "                    self.weights[i][j, k] += epsilon\n",
    "                    loss_plus = self._compute_loss(X, y)\n",
    "                    \n",
    "                    self.weights[i][j, k] -= 2 * epsilon\n",
    "                    loss_minus = self._compute_loss(X, y)\n",
    "                    \n",
    "                    self.weights[i][j, k] += epsilon\n",
    "                    \n",
    "                    grad = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "                    numerical_grads[j, k] = grad\n",
    "            \n",
    "            # Compare gradients\n",
    "            numerical_grads_list.extend(numerical_grads.ravel())\n",
    "            analytical_grads_list.extend(grads[\"dW\" + str(i)].ravel())\n",
    "            \n",
    "        # compare the difference between numerical and analytical gradients\n",
    "        numerical_grads_list = np.array(numerical_grads_list)\n",
    "        analytical_grads_list = np.array(analytical_grads_list)\n",
    "        \n",
    "        difference = np.linalg.norm(numerical_grads_list - analytical_grads_list) / \\\n",
    "            (np.linalg.norm(numerical_grads_list) + np.linalg.norm(analytical_grads_list))\n",
    "            \n",
    "        # print(f\"Gradient difference: {difference}\")\n",
    "        \n",
    "        if difference < 1e-7 or difference == 0 or np.isnan(difference):\n",
    "            print(\"Gradients are correct\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age  gender    income  education  married  children  occupation  \\\n",
      "0  0.586957     1.0  0.683976   0.666667      0.0  1.000000         0.2   \n",
      "1  0.130435     0.0  0.544222   0.333333      0.0  0.333333         0.1   \n",
      "2  0.586957     0.0  0.141740   0.000000      1.0  1.000000         0.3   \n",
      "3  0.021739     1.0  0.469873   1.000000      0.0  0.000000         0.6   \n",
      "4  0.239130     0.0  0.397630   0.666667      0.0  0.000000         0.1   \n",
      "\n",
      "   purchase_amount  most bought item  beauty  books  clothing  electronics  \\\n",
      "0         0.441351          0.695652       0      0         1            1   \n",
      "1         0.629936          0.565217       1      0         0            0   \n",
      "2         0.537555          0.130435       0      0         1            1   \n",
      "3         0.511921          0.608696       0      0         0            0   \n",
      "4         0.435510          0.173913       0      0         0            0   \n",
      "\n",
      "   food  furniture  home  sports  \n",
      "0     0          0     0       1  \n",
      "1     0          1     0       0  \n",
      "2     1          0     0       1  \n",
      "3     1          0     0       0  \n",
      "4     0          0     1       0  \n",
      "(800, 9) (100, 9) (100, 9)\n",
      "(800, 8) (100, 8) (100, 8)\n",
      "[0.56521739 1.         0.36267749 0.66666667 1.         0.66666667\n",
      " 0.         0.4625184  0.69565217] [1 1 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "filepath = \"../../data/interim/advertisement_normalized.csv\"\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# multiply income by 10\n",
    "df['income'] = df['income'] \n",
    "df['age'] = df['age'] \n",
    "df['purchase_amount'] = df['purchase_amount'] \n",
    "df = df.drop(columns=['city'])\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# save last 8 cols as target\n",
    "target = df.iloc[:, -8:]\n",
    "data = df.iloc[:, :-8]\n",
    "\n",
    "# shuffle the data\n",
    "np.random.seed(42)\n",
    "idx = np.random.permutation(data.index)\n",
    "\n",
    "X = data.iloc[idx].values\n",
    "y = target.iloc[idx].values\n",
    "\n",
    "# convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# split the data\n",
    "X_train = X[:int(0.8 * len(X))]\n",
    "X_test = X[int(0.8 * len(X)):int(0.9 * len(X))]\n",
    "X_val = X[int(0.9 * len(X)):]\n",
    "\n",
    "y_train = y[:int(0.8 * len(y))]\n",
    "y_test = y[int(0.8 * len(y)):int(0.9 * len(y))]\n",
    "y_val = y[int(0.9 * len(y)):]\n",
    "\n",
    "\n",
    "\n",
    "print(X_train.shape, X_test.shape, X_val.shape)\n",
    "print(y_train.shape, y_test.shape, y_val.shape)\n",
    "\n",
    "print(X_train[0], y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "mlp = MLP(input_size=X_train.shape[1], hidden_layers=[32, 32, 32], output_size=y_train.shape[1], epochs=1000, learning_rate=0.01, \n",
    "          batch_size=32, activation_function='relu', loss_function='cross_entropy', optimizer='mini_batch', Random_state=None, \n",
    "          weight_init='he', beta=0.9 ,model_type='class_ML', wandb_vaar=False, run_start=\"advertising\")\n",
    "\n",
    "# train the model\n",
    "mlp.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.951012998898352\n",
      "Accuracy: 5.25\n",
      "Hamming Loss: 0.34375\n",
      "Partial Accuracy: 0.65625\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss = mlp.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# predict\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "y_pred = mlp.filter_softmax(y_pred)\n",
    "\n",
    "# accuracy\n",
    "accuracy = mlp.accuracy(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "# # atleast one label per sample\n",
    "# for i in range(y_test.shape[0]):\n",
    "#     if \n",
    "\n",
    "# hamming loss\n",
    "hamming_loss = mlp.hamming_loss(y_test, y_pred)\n",
    "print(f\"Hamming Loss: {hamming_loss}\")\n",
    "\n",
    "# partial accuracy\n",
    "y_pred_partial = y_pred.reshape(-1, 1)\n",
    "y_test_partial = y_test.reshape(-1, 1)\n",
    "partial_accuracy = mlp.accuracy(y_test_partial, y_pred_partial)\n",
    "print(f\"Partial Accuracy: {partial_accuracy}\")\n",
    "\n",
    "# plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(mlp.loss)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[63  0]\n",
      "  [36  1]]\n",
      "\n",
      " [[64  3]\n",
      "  [29  4]]\n",
      "\n",
      " [[60  5]\n",
      "  [28  7]]\n",
      "\n",
      " [[61  3]\n",
      "  [34  2]]\n",
      "\n",
      " [[61  2]\n",
      "  [37  0]]\n",
      "\n",
      " [[76  0]\n",
      "  [24  0]]\n",
      "\n",
      " [[61  0]\n",
      "  [38  1]]\n",
      "\n",
      " [[54 12]\n",
      "  [24 10]]]\n",
      "Micro-averaged Precision: 0.2\n",
      "Micro-averaged Recall: 0.7142857142857143\n",
      "Micro-averaged F1-Score: 0.3125\n"
     ]
    }
   ],
   "source": [
    "def multi_label_confusion_matrix(y_true, y_pred):\n",
    "    num_labels = y_true.shape[1]\n",
    "    confusion_matrices = []\n",
    "\n",
    "    for i in range(num_labels):\n",
    "        tp = np.sum((y_pred[:, i] == 1) & (y_true[:, i] == 1))  # True Positive\n",
    "        tn = np.sum((y_pred[:, i] == 0) & (y_true[:, i] == 0))  # True Negative\n",
    "        fp = np.sum((y_pred[:, i] == 1) & (y_true[:, i] == 0))  # False Positive\n",
    "        fn = np.sum((y_pred[:, i] == 0) & (y_true[:, i] == 1))  # False Negative\n",
    "\n",
    "        matrix = np.array([[tn, fp], [fn, tp]])\n",
    "        confusion_matrices.append(matrix)\n",
    "\n",
    "    return np.array(confusion_matrices)\n",
    "\n",
    "# confusion matrix\n",
    "confusion_matrix = multi_label_confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "\n",
    "# micro-averaged precision, recall, f1_score\n",
    "micro_precision = np.sum(confusion_matrix[:, 0, 0]) / np.sum(confusion_matrix[:, 0, 0] + np.sum(confusion_matrix[:, 1, 0]))\n",
    "micro_recall = np.sum(confusion_matrix[:, 0, 0]) / np.sum(confusion_matrix[:, 0, 0] + np.sum(confusion_matrix[:, 1, 1]))\n",
    "micro_f1_score = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall)\n",
    "\n",
    "print(f\"Micro-averaged Precision: {micro_precision}\")\n",
    "print(f\"Micro-averaged Recall: {micro_recall}\")\n",
    "print(f\"Micro-averaged F1-Score: {micro_f1_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: [1.         0.57142857 0.58333333 0.4        0.         0.\n",
      " 1.         0.45454545]\n",
      "Recall: [0.02702703 0.12121212 0.2        0.05555556 0.         0.\n",
      " 0.02564103 0.29411765]\n",
      "F1-Score: [0.05263158 0.2        0.29787234 0.09756098 0.         0.\n",
      " 0.05       0.35714286]\n",
      "Macro Precision: 0.5011634196345568\n",
      "Macro Recall: 0.090444172059198\n",
      "Macro F1-Score: 0.13190096882748073\n"
     ]
    }
   ],
   "source": [
    "def precision_recall_f1(y_true, y_pred):\n",
    "    epsilon = 1e-9  # To avoid division by zero\n",
    "\n",
    "    # True Positives, False Positives, and False Negatives for each label\n",
    "    true_positives = np.sum((y_pred == 1) & (y_true == 1), axis=0)\n",
    "    false_positives = np.sum((y_pred == 1) & (y_true == 0), axis=0)\n",
    "    false_negatives = np.sum((y_pred == 0) & (y_true == 1), axis=0)\n",
    "\n",
    "    # Precision: TP / (TP + FP)\n",
    "    precision = true_positives / (true_positives + false_positives + epsilon)\n",
    "    \n",
    "    # Recall: TP / (TP + FN)\n",
    "    recall = true_positives / (true_positives + false_negatives + epsilon)\n",
    "    \n",
    "    # F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "\n",
    "    # Return macro-average (mean over all labels)\n",
    "    return precision, recall, f1\n",
    "\n",
    "precision, recall, f1 = precision_recall_f1(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "\n",
    "# macro-average\n",
    "macro_precision = np.mean(precision)\n",
    "macro_recall = np.mean(recall)\n",
    "macro_f1 = np.mean(f1)\n",
    "\n",
    "print(f\"Macro Precision: {macro_precision}\")\n",
    "print(f\"Macro Recall: {macro_recall}\")\n",
    "print(f\"Macro F1-Score: {macro_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-label Accuracy: 0.01\n"
     ]
    }
   ],
   "source": [
    "def multi_label_accuracy(y_true, y_pred):\n",
    "    # Exact match accuracy (where all labels for a sample are correctly predicted)\n",
    "    correct_predictions = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "    total_predictions = len(y_true)\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "accuracy = multi_label_accuracy(y_test, y_pred)\n",
    "print(f\"Multi-label Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 0 0 0 1] [0 0 0 0 0 0 0 1]\n",
      "[ True False  True False  True  True  True  True]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(y_test[1] , y_pred[1])\n",
    "print((y_test[1] == y_pred[1]))\n",
    "print(np.sum(y_test[1] == y_pred[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
