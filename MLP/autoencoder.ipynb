{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "''' MLP class  '''\n",
    "class MLP:\n",
    "    ''' Multi-layer perceptron class '''\n",
    "    ''' input_size, hidden_layers = [sizes of the hidden layers], output_size, epochs, learning_rate, batch_size, activation_function, loss_function '''\n",
    "    def __init__(self, input_size, hidden_layers, output_size, epochs = 100, learning_rate = 0.01, early_stopping = False,\n",
    "                 batch_size = 32, activation_function = 'relu', loss_function = 'mse', optimizer = 'sgd', beta = 0.0, \n",
    "                 Random_state = None, weight_init = 'he', model_type = 'class_ML', wandb_vaar = False, run_start = \"hyperparam_tuning\", wandb_finish = True):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_size = output_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.early_stopping = early_stopping\n",
    "        self.beta = beta\n",
    "        self.weight_init = weight_init\n",
    "        self.model_type = model_type\n",
    "        self.wandb_vaar = wandb_vaar\n",
    "        self.wandb_finish = wandb_finish\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.loss = []\n",
    "        self.history = []\n",
    "        \n",
    "        np.random.seed(Random_state)\n",
    "        \n",
    "        self._initialize_weights()\n",
    "        \n",
    "        run_name = f\"{run_start}-{self.activation_function}-{self.optimizer}-{self.loss_function}-{self.weight_init}-{self.epochs}-{self.learning_rate}-{self.batch_size}\"\n",
    "        \n",
    "        # Initialize WandB\n",
    "        if wandb_vaar:\n",
    "            wandb.init(\n",
    "                project=\"SMAI_A3-MLP\", \n",
    "                name=run_name,\n",
    "                config={\n",
    "                \"learning_rate\": self.learning_rate,\n",
    "                \"epochs\": self.epochs,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"activation\": self.activation_function,\n",
    "                \"optimizer\": self.optimizer,\n",
    "                \"input_size\": self.input_size,\n",
    "                \"hidden_layer_sizes\": self.hidden_layers,\n",
    "                \"output_size\": self.output_size,\n",
    "                \"loss_function\": self.loss_function,\n",
    "                \"weight_init\": self.weight_init,\n",
    "                \"Random_state\": Random_state,\n",
    "                \"beta\": self.beta,\n",
    "                \"early_stopping\": self.early_stopping,\n",
    "            })\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        ''' Initialize weights '''\n",
    "        layers = [self.input_size] + self.hidden_layers + [self.output_size]\n",
    "        for i in range(len(layers) - 1):\n",
    "            if self.weight_init == 'random':\n",
    "                self.weights.append(np.random.randn(layers[i], layers[i+1]))  # Random initialization\n",
    "            \n",
    "            elif self.weight_init == 'he':\n",
    "                self.weights.append(np.random.randn(layers[i], layers[i+1]) * np.sqrt(2. / layers[i]))  # He initialization\n",
    "            \n",
    "            elif self.weight_init == 'xavier':\n",
    "                limit = np.sqrt(6 / (layers[i] + layers[i+1]))\n",
    "                self.weights.append(np.random.uniform(-limit, limit, (layers[i], layers[i+1])))  # Xavier/Glorot initialization\n",
    "            \n",
    "            self.biases.append(np.zeros(layers[i+1]))\n",
    "        \n",
    "        self.vW = [np.zeros_like(w) for w in self.weights]  # Initialize velocity for weights\n",
    "        self.vb = [np.zeros_like(b) for b in self.biases]   # Initialize velocity for biases\n",
    "    \n",
    "    # Activation functions       \n",
    "    def relu(self, x):\n",
    "        ''' ReLU activation function '''\n",
    "        return np.where(x > 0, x, x * 0.01)\n",
    "    def relu_derivative(self, x):\n",
    "        ''' ReLU derivative '''\n",
    "        return np.where(x > 0, 1, 0.01)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        ''' Sigmoid activation function '''\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    def sigmoid_derivative(self, x):\n",
    "        ''' Sigmoid derivative '''\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        ''' Tanh activation function '''\n",
    "        return np.tanh(x)\n",
    "    def tanh_derivative(self, x):\n",
    "        ''' Tanh derivative '''\n",
    "        return 1 - np.tanh(x)**2\n",
    "    \n",
    "    def linear(self, x):\n",
    "        ''' Linear activation function '''\n",
    "        return x\n",
    "    def linear_derivative(self, x):\n",
    "        ''' Linear derivative '''\n",
    "        return 1\n",
    "            \n",
    "    def _activation(self, activation_function = None):\n",
    "        ''' Activation function '''\n",
    "        if activation_function == 'relu':\n",
    "            return self.relu, self.relu_derivative\n",
    "        elif activation_function == 'sigmoid':\n",
    "            return self.sigmoid, self.sigmoid_derivative\n",
    "        elif activation_function == 'tanh':\n",
    "            return self.tanh, self.tanh_derivative\n",
    "        elif activation_function == 'linear':\n",
    "            return self.linear, self.linear_derivative\n",
    "        else:\n",
    "            raise ValueError('Activation function not supported')\n",
    "    \n",
    "    # Optimizers\n",
    "    def mse(self, y, y_pred):\n",
    "        ''' Mean squared error '''\n",
    "        return np.mean((y - y_pred)**2)\n",
    "    def mse_derivative(self, y, y_pred):\n",
    "        ''' Mean squared error derivative '''\n",
    "        return 2*(y_pred - y)\n",
    "    \n",
    "    # Loss for regression\n",
    "    def rmse(self, y, y_pred):\n",
    "        ''' Root mean squared error '''\n",
    "        return np.sqrt(np.mean((y - y_pred)**2))\n",
    "    def mae(self, y, y_pred):\n",
    "        ''' Mean absolute error '''\n",
    "        return np.mean(np.abs(y - y_pred))\n",
    "    def r_squared(self, y, y_pred):\n",
    "        ''' R-squared metric '''\n",
    "        ss_res = np.sum((y - y_pred)**2)\n",
    "        ss_tot = np.sum((y - np.mean(y))**2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    def cross_entropy(self, y, y_pred):\n",
    "        ''' Cross entropy loss '''\n",
    "        return -np.sum(y * np.log(y_pred)) / len(y)\n",
    "    \n",
    "    def cross_entropy_derivative(self, y, y_pred):\n",
    "        ''' Cross entropy derivative '''\n",
    "        return (y_pred - y) # / (y_pred * (1 - y_pred) + 1e-6)  \n",
    "    \n",
    "    def _loss(self):\n",
    "        ''' Loss function '''\n",
    "        if self.loss_function == 'mse':\n",
    "            return self.mse, self.mse_derivative\n",
    "        elif self.loss_function == 'cross_entropy':\n",
    "            return self.cross_entropy, self.cross_entropy_derivative\n",
    "        else:\n",
    "            raise ValueError('Loss function not supported')\n",
    "        \n",
    "    def _softmax(self, x):\n",
    "        ''' Softmax activation function '''\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "    \n",
    "    def _one_hot(self, y):\n",
    "        ''' One-hot encoding '''\n",
    "        print(y.min(), y.max())\n",
    "        one_hot = np.zeros((y.size, y.max()+1- y.min()))\n",
    "        for i in range(y.size):\n",
    "            one_hot[i, y[i] - y.min()] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def filter_onehot(self, y_true, y_test):\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "        return y_true, y_test\n",
    "    \n",
    "    def filter_softmax(self, y_pred):\n",
    "        # if >0.5, then 1 else 0\n",
    "        return np.where(y_pred > 0.5, 1, 0)\n",
    "        \n",
    "    \n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct_predictions = np.sum(y_true == y_pred)\n",
    "        total_predictions = len(y_true)\n",
    "        return correct_predictions / total_predictions\n",
    "    \n",
    "    def hamming_loss(self, y_true, y_pred):\n",
    "        # Compute the number of incorrectly predicted labels\n",
    "        incorrect_labels = np.sum(y_true != y_pred)\n",
    "        \n",
    "        # Total number of labels is the number of samples times the number of labels per sample\n",
    "        total_labels = y_true.shape[0] * y_true.shape[1]\n",
    "        \n",
    "        # Hamming Loss: Proportion of incorrect labels\n",
    "        return incorrect_labels / total_labels\n",
    "    \n",
    "    def multi_label_partial_accuracy(self, y_true, y_pred):\n",
    "        y_pred_partial = y_pred.reshape(-1, 1)\n",
    "        y_true_partial = y_true.reshape(-1, 1)\n",
    "        partial_accuracy = self.accuracy(y_true_partial, y_pred_partial)\n",
    "        return partial_accuracy\n",
    "    \n",
    "    def multi_label_accuracy(self, y_true, y_pred):\n",
    "        # Exact match accuracy (where all labels for a sample are correctly predicted)\n",
    "        correct_predictions = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "        total_predictions = len(y_true)\n",
    "        return correct_predictions / total_predictions\n",
    "    \n",
    "    def multi_label_confusion_matrix(self, y_true, y_pred):\n",
    "        num_labels = y_true.shape[1]\n",
    "        confusion_matrices = []\n",
    "\n",
    "        for i in range(num_labels):\n",
    "            tp = np.sum((y_pred[:, i] == 1) & (y_true[:, i] == 1))  # True Positive\n",
    "            tn = np.sum((y_pred[:, i] == 0) & (y_true[:, i] == 0))  # True Negative\n",
    "            fp = np.sum((y_pred[:, i] == 1) & (y_true[:, i] == 0))  # False Positive\n",
    "            fn = np.sum((y_pred[:, i] == 0) & (y_true[:, i] == 1))  # False Negative\n",
    "\n",
    "            matrix = np.array([[tn, fp], [fn, tp]])\n",
    "            confusion_matrices.append(matrix)\n",
    "\n",
    "        return np.array(confusion_matrices)\n",
    "    \n",
    "    def multi_label_micro_metrics_partial(self, y_true, y_pred):\n",
    "        # confusion matrix\n",
    "        confusion_matrix = self.multi_label_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        # micro-averaged precision, recall, f1_score\n",
    "        micro_precision = np.sum(confusion_matrix[:, 0, 0]) / np.sum(confusion_matrix[:, 0, 0] + np.sum(confusion_matrix[:, 1, 0] + 1e-6))\n",
    "        micro_recall = np.sum(confusion_matrix[:, 0, 0]) / np.sum(confusion_matrix[:, 0, 0] + np.sum(confusion_matrix[:, 1, 1] + 1e-6))\n",
    "        micro_f1_score = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall)\n",
    "        \n",
    "        return micro_precision, micro_recall, micro_f1_score\n",
    "    \n",
    "    def multi_label_macro_metrics_partial(self, y_true, y_pred):\n",
    "        # confusion matrix\n",
    "        confusion_matrix = self.multi_label_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        # macro-averaged precision, recall, f1_score\n",
    "        macro_precision = np.mean(confusion_matrix[:, 0, 0] / (confusion_matrix[:, 0, 0] + confusion_matrix[:, 1, 0] + 1e-6))\n",
    "        macro_recall = np.mean(confusion_matrix[:, 0, 0] / (confusion_matrix[:, 0, 0] + confusion_matrix[:, 1, 1] + 1e-6))\n",
    "        macro_f1_score = 2 * (macro_precision * macro_recall) / (macro_precision + macro_recall)\n",
    "        \n",
    "        return macro_precision, macro_recall, macro_f1_score\n",
    "    \n",
    "    def multi_label_precision_recall_f1(self, y_true, y_pred):\n",
    "        epsilon = 1e-9  # To avoid division by zero\n",
    "\n",
    "        # True Positives, False Positives, and False Negatives for each label\n",
    "        true_positives = np.sum((y_pred == 1) & (y_true == 1), axis=0)\n",
    "        false_positives = np.sum((y_pred == 1) & (y_true == 0), axis=0)\n",
    "        false_negatives = np.sum((y_pred == 0) & (y_true == 1), axis=0)\n",
    "\n",
    "        # Precision: TP / (TP + FP)\n",
    "        precision = true_positives / (true_positives + false_positives + epsilon)\n",
    "        \n",
    "        # Recall: TP / (TP + FN)\n",
    "        recall = true_positives / (true_positives + false_negatives + epsilon)\n",
    "        \n",
    "        # F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "\n",
    "        # Return macro-average (mean over all labels)\n",
    "        return precision, recall, f1\n",
    "    \n",
    "    def confusion_matrix(self, y_true, y_pred):\n",
    "        classes = np.unique(y_true)\n",
    "        matrix = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "\n",
    "        for i in range(len(y_true)):\n",
    "            true_idx = np.where(classes == y_true[i])[0][0]\n",
    "            pred_idx = np.where(classes == y_pred[i])[0][0]\n",
    "            matrix[true_idx, pred_idx] += 1\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def recall(self, y_true, y_pred):\n",
    "        matrix = self.confusion_matrix(y_true, y_pred)\n",
    "        recall_values = np.diag(matrix) / np.sum(matrix, axis=1)  # TP / (TP + FN)\n",
    "        recall_values = np.nan_to_num(recall_values)  # Handle division by zero\n",
    "        return np.mean(recall_values)\n",
    "    \n",
    "    def precision(self, y_true, y_pred):\n",
    "        matrix = self.confusion_matrix(y_true, y_pred)\n",
    "        precision_values = np.diag(matrix) / np.sum(matrix, axis=0)  # TP / (TP + FP)\n",
    "        precision_values = np.nan_to_num(precision_values)  # Handle division by zero\n",
    "        return np.mean(precision_values)\n",
    "    \n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        prec = self.precision(y_true, y_pred)\n",
    "        rec = self.recall(y_true, y_pred)\n",
    "        if (prec + rec) == 0:\n",
    "            return 0\n",
    "        return 2 * (prec * rec) / (prec + rec)\n",
    "    \n",
    "    def model_functions(self):\n",
    "        if self.model_type == 'class_ML':\n",
    "            return self.sigmoid, self.sigmoid_derivative\n",
    "        elif self.model_type == 'class_MC':\n",
    "            return self._softmax, self.cross_entropy_derivative\n",
    "        elif self.model_type == 'regression':\n",
    "            return self._activation(self.activation_function)\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self, X, latent_space = False):\n",
    "        ''' Forward pass '''\n",
    "        activations = [X]\n",
    "        Z = [X]\n",
    "        \n",
    "        activation, _ = self._activation(self.activation_function)\n",
    "        current_activation = X\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(current_activation, self.weights[i]) + self.biases[i]\n",
    "            Z.append(z)\n",
    "            current_activation = activation(z)\n",
    "            activations.append(current_activation)\n",
    "            \n",
    "        # Output layer\n",
    "        z = np.dot(current_activation, self.weights[-1]) + self.biases[-1]\n",
    "        Z.append(z)\n",
    "        \n",
    "        # Check if the model is for classification or regression\n",
    "        activation_function, _ = self.model_functions()\n",
    "        # output_activation = self.relu(z) # Output activation function\n",
    "        output_activation = activation_function(z) # Output activation function\n",
    "        activations.append(output_activation)\n",
    "        self.activations = activations\n",
    "        \n",
    "        return Z, activations\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, X, y, Z, activations):\n",
    "        ''' Backward pass '''\n",
    "        grads = {}\n",
    "        m = y.shape[0]\n",
    "        \n",
    "        # Activation function\n",
    "        # activation_function, activation_derivative = self._activation(self.activation_function)\n",
    "        activation_function, activation_derivative = self.model_functions()\n",
    "        \n",
    "        # Loss function\n",
    "        # loss, loss_derivative = self._loss()\n",
    "        if self.model_type == 'class_ML':\n",
    "            loss, loss_derivative = self.cross_entropy, self.cross_entropy_derivative\n",
    "        elif self.model_type == 'class_MC':\n",
    "            loss, loss_derivative = self.cross_entropy, self.cross_entropy_derivative\n",
    "        elif self.model_type == 'regression':\n",
    "            loss, loss_derivative = self.mse, self.mse_derivative\n",
    "        \n",
    "        # backprop loss in output layer\n",
    "        dA = loss_derivative(y, (activations[-1]))\n",
    "        if self.model_type == 'class_MC':\n",
    "            dZ = (activations[-1] - y) / m\n",
    "        else:\n",
    "            dZ = dA * activation_derivative(Z[-1])\n",
    "        \n",
    "        grads[\"dW\" + str(len(self.weights)-1)] = np.dot(activations[-2].T, dZ) / m\n",
    "        grads[\"db\" + str(len(self.weights)-1)] = np.sum(dZ, axis=0) / m\n",
    "        self.dZ = dZ\n",
    "        \n",
    "        # backprop hidden layers\n",
    "        activation_function, activation_derivative = self._activation(self.activation_function)\n",
    "        for i in range(len(self.weights)-2, -1, -1):\n",
    "            dA = np.dot(dZ, self.weights[i+1].T)\n",
    "            dZ = dA * activation_derivative(Z[i+1])\n",
    "            grads[\"dW\" + str(i)] = np.dot(activations[i].T, dZ) / m\n",
    "            grads[\"db\" + str(i)] = np.sum(dZ, axis=0) / m\n",
    "        \n",
    "        self.dZ = dZ\n",
    "        return grads\n",
    "    \n",
    "    # # Update weights\n",
    "    # def update_weights(self, grads):\n",
    "    #     ''' Update weights '''\n",
    "    #     # print('weights:', len(self.weights))\n",
    "    #     # for i in range(len(self.weights)):\n",
    "    #     #     print(self.weights[i].shape, grads[\"dW\" + str(i)].shape)\n",
    "    #     for i in range(len(self.weights)):\n",
    "    #         self.weights[i] -= self.learning_rate * grads[\"dW\" + str(i)]\n",
    "    #         self.biases[i] -= self.learning_rate * grads[\"db\" + str(i)]\n",
    "    \n",
    "    # update weights using momentum\n",
    "    def update_weights(self, grads):\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update the velocity for weights and biases using momentum\n",
    "            self.vW[i] = self.beta * self.vW[i] + (1 - self.beta) * grads[\"dW\" + str(i)]\n",
    "            self.vb[i] = self.beta * self.vb[i] + (1 - self.beta) * grads[\"db\" + str(i)]\n",
    "            \n",
    "            # Update the weights and biases using velocity\n",
    "            self.weights[i] -= self.learning_rate * self.vW[i]\n",
    "            self.biases[i] -= self.learning_rate * self.vb[i]\n",
    "            \n",
    "    # Train the model\n",
    "    def fit(self, X, y, X_val = None, y_val = None):\n",
    "        # encode the target variable if it is not regression\n",
    "        # if not self.model_type == 'regression':\n",
    "        #     y = self._one_hot(y)\n",
    "        \n",
    "        ''' Train the model '''\n",
    "        for epoch in range(self.epochs):\n",
    "            self.optimize(X, y)\n",
    "            print(f\"Epoch: {epoch+1}\")\n",
    "            \n",
    "            # Loss\n",
    "            loss_func, loss_derivative = self._loss()\n",
    "            y_pred = self.forward(X)[1][-1] # Predictions\n",
    "            self.loss.append(loss_func(y, y_pred))\n",
    "            \n",
    "            # Loss on validation set\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_pred_val = self.predict(X_val)\n",
    "                self.history.append(loss_func(y_val, y_pred_val))\n",
    "            \n",
    "            # Log accuracy, recall, precision, f1_score\n",
    "            if self.model_type == 'class_MC' and self.wandb_vaar:\n",
    "                # train\n",
    "                y_true, y_pred = self.filter_onehot(y, y_pred)\n",
    "                accuracy = self.accuracy(y_true, y_pred)\n",
    "                recall = self.recall(y_true, y_pred)\n",
    "                precision = self.precision(y_true, y_pred)\n",
    "                f1_score = self.f1_score(y_true, y_pred)\n",
    "                # val\n",
    "                y_true_val, y_pred_val = self.filter_onehot(y_val, y_pred_val)\n",
    "                accuracy_val = self.accuracy(y_true_val, y_pred_val)\n",
    "                recall_val = self.recall(y_true_val, y_pred_val)\n",
    "                precision_val = self.precision(y_true_val, y_pred_val)\n",
    "                f1_score_val = self.f1_score(y_true_val, y_pred_val)\n",
    "            elif self.model_type == 'class_ML' and self.wandb_vaar:\n",
    "                # train\n",
    "                y_pred = self.filter_softmax(y_pred)\n",
    "                accuracy = self.accuracy(y, y_pred)\n",
    "                partial_accuracy = self.multi_label_partial_accuracy(y, y_pred)\n",
    "                micro_precision, micro_recall, micro_f1_score = self.multi_label_micro_metrics_partial(y, y_pred)\n",
    "                macro_precision, macro_recall, macro_f1_score = self.multi_label_macro_metrics_partial(y, y_pred)\n",
    "                hamming_loss = self.hamming_loss(y, y_pred)\n",
    "                multi_label_accuracy = self.multi_label_accuracy(y, y_pred)\n",
    "                # val\n",
    "                y_pred_val = self.filter_softmax(y_pred_val)\n",
    "                accuracy_val = self.accuracy(y_val, y_pred_val)\n",
    "                partial_accuracy_val = self.multi_label_partial_accuracy(y_val, y_pred_val)\n",
    "                micro_precision_val, micro_recall_val, micro_f1_score_val = self.multi_label_micro_metrics_partial(y_val, y_pred_val)\n",
    "                macro_precision_val, macro_recall_val, macro_f1_score_val = self.multi_label_macro_metrics_partial(y_val, y_pred_val)\n",
    "                hamming_loss_val = self.hamming_loss(y_val, y_pred_val)\n",
    "                multi_label_accuracy_val = self.multi_label_accuracy(y_val, y_pred_val)\n",
    "            elif self.model_type == 'regression' and self.wandb_vaar:\n",
    "                # train\n",
    "                loss = self.mse(y, y_pred)\n",
    "                r_squared = self.r_squared(y, y_pred)\n",
    "                mean_absolute_error = self.mae(y, y_pred)\n",
    "                rmse = self.rmse(y, y_pred)\n",
    "                # # val\n",
    "                # loss_val = self.mse(y_val, y_pred_val)\n",
    "                # r_squared_val = self.r_squared(y_val, y_pred_val)\n",
    "                # mean_absolute_error_val = self.mae(y_val, y_pred_val)\n",
    "                # rmse_val = self.rmse(y_val, y_pred_val)\n",
    "                \n",
    "            \n",
    "            if self.wandb_vaar and self.wandb_vaar and self.model_type == 'class_MC':\n",
    "                wandb.log({\"epoch\":epoch+1, \"train/loss\": self.loss[-1], \"train/accuracy\": accuracy, \"train/recall\": recall, \"train/precision\": precision, \n",
    "                    \"train/f1_score\": f1_score, \"val/loss\": self.history[-1], \"val/accuracy\": accuracy_val, \"val/recall\": recall_val, \"val/precision\": precision_val, \n",
    "                    \"val/f1_score\": f1_score_val})\n",
    "            elif self.wandb_vaar and self.wandb_vaar and self.model_type == 'class_ML':\n",
    "                wandb.log({\"epoch\":epoch+1, \"train/loss\": self.loss[-1], \"train/accuracy\": accuracy, \"train/partial_accuracy\": partial_accuracy, \n",
    "                    \"train/micro_precision\": micro_precision, \"train/micro_recall\": micro_recall, \"train/micro_f1_score\": micro_f1_score, \n",
    "                    \"train/macro_precision\": macro_precision, \"train/macro_recall\": macro_recall, \"train/macro_f1_score\": macro_f1_score, \n",
    "                    \"train/hamming_loss\": hamming_loss, \"train/multi_label_accuracy\": multi_label_accuracy, \n",
    "                    \"val/loss\": self.history[-1], \"val/accuracy\": accuracy_val, \"val/partial_accuracy\": partial_accuracy_val, \n",
    "                    \"val/micro_precision\": micro_precision_val, \"val/micro_recall\": micro_recall_val, \"val/micro_f1_score\": micro_f1_score_val, \n",
    "                    \"val/macro_precision\": macro_precision_val, \"val/macro_recall\": macro_recall_val, \"val/macro_f1_score\": macro_f1_score_val, \n",
    "                    \"val/hamming_loss\": hamming_loss_val, \"val/multi_label_accuracy\": multi_label_accuracy_val})\n",
    "            elif self.wandb_vaar and self.model_type == 'regression':\n",
    "                wandb.log({\"epoch\":epoch+1, \"train/loss\": self.loss[-1], \"train/r_squared\": r_squared, \"train/mean_absolute_error\": mean_absolute_error,\"train/rmse\": rmse,})\n",
    "                        #    \"val/loss\": self.history[-1], \"val/r_squared\": r_squared_val, \"val/mean_absolute_error\": mean_absolute_error_val, \"val/rmse\": rmse_val})\n",
    "            \n",
    "            if self.early_stopping:\n",
    "                if len(self.loss) > 1 and abs(self.loss[-1] - self.loss[-2]) < 1e-6:\n",
    "                    print(f\"Converged after {epoch+1} epochs\")\n",
    "                    break\n",
    "        \n",
    "        if self.wandb_vaar and self.wandb_finish:\n",
    "            wandb.finish()\n",
    "            \n",
    "    # Optimize\n",
    "    def optimize(self, X, y):\n",
    "        ''' Optimize the model '''\n",
    "        if self.optimizer == 'sgd':\n",
    "            self.sgd(X, y)\n",
    "        elif self.optimizer == 'mini_batch':\n",
    "            self.mini_batch(X, y)\n",
    "        elif self.optimizer == 'full_batch':\n",
    "            self.batch(X, y)\n",
    "        else:\n",
    "            raise ValueError('Optimizer not supported')\n",
    "    \n",
    "    def sgd(self, X, y):\n",
    "        ''' Stochastic gradient descent '''\n",
    "        for i in range(X.shape[0]):\n",
    "            Z, activations = self.forward(X[i])\n",
    "            \n",
    "            # Adjust the sizes of matrix\n",
    "            for j in range(len(activations)):\n",
    "                activations[j] = activations[j].reshape(1, -1)\n",
    "                Z[j] = Z[j].reshape(1, -1)\n",
    "            \n",
    "            grads = self.backward(X[i], y[i].reshape(1,-1), Z, activations)\n",
    "            self.update_weights(grads)\n",
    "    \n",
    "    def mini_batch(self, X, y):\n",
    "        ''' Mini-batch gradient descent '''\n",
    "        for i in range(0, X.shape[0], self.batch_size):\n",
    "            Z, activations = self.forward(X[i:i+self.batch_size])\n",
    "            grads = self.backward(X[i:i+self.batch_size], y[i:i+self.batch_size], Z, activations)\n",
    "            self.update_weights(grads)\n",
    "            \n",
    "    def batch(self, X, y):\n",
    "        ''' Batch gradient descent '''\n",
    "        Z, activations = self.forward(X)\n",
    "        grads = self.backward(X, y, Z, activations)\n",
    "        self.update_weights(grads)\n",
    "        \n",
    "    # Predict\n",
    "    def predict(self, X):\n",
    "        ''' Predict '''\n",
    "        Z, activations = self.forward(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def latent_space(self, X):\n",
    "        ''' Latent space '''\n",
    "        Z, activations = self.forward(X)\n",
    "        return Z[-1]\n",
    "    \n",
    "    # Evaluate\n",
    "    def evaluate(self, X, y):\n",
    "        ''' Evaluate '''\n",
    "        y_pred = self.predict(X)\n",
    "        loss, _ = self._loss()\n",
    "        return loss(y, y_pred)\n",
    "    \n",
    "    def _compute_loss(self, X, y):\n",
    "        ''' Compute loss '''\n",
    "        Z, activations = self.forward(X)\n",
    "        loss, _ = self._loss()\n",
    "        return loss(y, activations[-1])\n",
    "    \n",
    "    # gradient checks\n",
    "    def check_gradients(self, X, y):\n",
    "        ''' Gradient checks '''\n",
    "        epsilon = 1e-4\n",
    "        Z, activations = self.forward(X)\n",
    "        grads = self.backward(X, y, Z, activations)\n",
    "        \n",
    "        numerical_grads_list = []\n",
    "        analytical_grads_list = []\n",
    "        \n",
    "        # Check gradients\n",
    "        for i in range(len(self.weights)):\n",
    "            numerical_grads = np.zeros_like(self.weights[i])\n",
    "            for j in range(self.weights[i].shape[0]):\n",
    "                for k in range(self.weights[i].shape[1]):\n",
    "                    self.weights[i][j, k] += epsilon\n",
    "                    loss_plus = self._compute_loss(X, y)\n",
    "                    \n",
    "                    self.weights[i][j, k] -= 2 * epsilon\n",
    "                    loss_minus = self._compute_loss(X, y)\n",
    "                    \n",
    "                    self.weights[i][j, k] += epsilon\n",
    "                    \n",
    "                    grad = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "                    numerical_grads[j, k] = grad\n",
    "            \n",
    "            # Compare gradients\n",
    "            numerical_grads_list.extend(numerical_grads.ravel())\n",
    "            analytical_grads_list.extend(grads[\"dW\" + str(i)].ravel())\n",
    "            \n",
    "        # compare the difference between numerical and analytical gradients\n",
    "        numerical_grads_list = np.array(numerical_grads_list)\n",
    "        analytical_grads_list = np.array(analytical_grads_list)\n",
    "        \n",
    "        difference = np.linalg.norm(numerical_grads_list - analytical_grads_list) / \\\n",
    "            (np.linalg.norm(numerical_grads_list) + np.linalg.norm(analytical_grads_list))\n",
    "            \n",
    "        print(f\"Gradient difference: {difference}\")\n",
    "        \n",
    "        if difference < 1e-7 or difference == 0 or np.isnan(difference):\n",
    "            print(\"Gradients are correct\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "class Autoencoder:\n",
    "    def __init__(self, input_size, hidden_layers, latent_dim, epochs=100, learning_rate=0.01, \n",
    "                 batch_size=32, activation_function='relu', loss_function='mse', \n",
    "                 optimizer='sgd', beta=0.0, Random_state=None, weight_init='he', \n",
    "                 wandb_vaar=False, run_start=\"autoencoder_training\", wandb_finish=False):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = latent_dim\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.beta = beta\n",
    "        self.weight_init = weight_init\n",
    "        self.wandb_vaar = wandb_vaar\n",
    "        self.wandb_finish = wandb_finish\n",
    "\n",
    "        # Create encoder\n",
    "        self.encoder = MLP(input_size, hidden_layers, latent_dim, epochs, learning_rate,\n",
    "                           False, batch_size, activation_function, loss_function,\n",
    "                           optimizer, beta, Random_state, weight_init, 'regression', False)\n",
    "\n",
    "        # Create decoder (reverse of encoder)\n",
    "        decoder_layers = hidden_layers[::-1]\n",
    "        self.decoder = MLP(latent_dim, decoder_layers, input_size, epochs, learning_rate,\n",
    "                           False, batch_size, activation_function, loss_function,\n",
    "                           optimizer, beta, Random_state, weight_init, 'regression', False)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        latent_Z, latent_activations = self.encoder.forward(X)\n",
    "        reconstructed_Z, reconstructed_activations = self.decoder.forward(latent_activations[-1])\n",
    "        return latent_Z, latent_activations, reconstructed_Z, reconstructed_activations\n",
    "\n",
    "    def backward(self, X, latent_Z, latent_activations, reconstructed_Z, reconstructed_activations):\n",
    "        # Compute loss\n",
    "        loss, loss_derivative = self._loss()\n",
    "        loss_value = loss(X, reconstructed_activations[-1])\n",
    "        \n",
    "        # Backpropagate through decoder\n",
    "        decoder_grads = self.decoder.backward(latent_activations[-1], X, reconstructed_Z, reconstructed_activations)\n",
    "        \n",
    "        # Backpropagate through encoder\n",
    "        dZ = np.dot(self.decoder.dZ, self.decoder.weights[0].T)\n",
    "        encoder_grads = self.encoder.backward(X, dZ, latent_Z, latent_activations)\n",
    "        \n",
    "        return encoder_grads, decoder_grads, loss_value\n",
    "\n",
    "    def update_weights(self, encoder_grads, decoder_grads):\n",
    "        self.encoder.update_weights(encoder_grads)\n",
    "        self.decoder.update_weights(decoder_grads)\n",
    "\n",
    "    def fit(self, X, X_val=None):\n",
    "        for epoch in range(self.epochs):\n",
    "            if self.optimizer == 'sgd':\n",
    "                for i in range(X.shape[0]):\n",
    "                    latent_Z, latent_activations, reconstructed_Z, reconstructed_activations = self.forward(X[i:i+1])\n",
    "                    encoder_grads, decoder_grads, loss = self.backward(X[i:i+1], latent_Z, latent_activations, reconstructed_Z, reconstructed_activations)\n",
    "                    self.update_weights(encoder_grads, decoder_grads)\n",
    "            elif self.optimizer == 'mini_batch':\n",
    "                for i in range(0, X.shape[0], self.batch_size):\n",
    "                    batch = X[i:i+self.batch_size]\n",
    "                    latent_Z, latent_activations, reconstructed_Z, reconstructed_activations = self.forward(batch)\n",
    "                    encoder_grads, decoder_grads, loss = self.backward(batch, latent_Z, latent_activations, reconstructed_Z, reconstructed_activations)\n",
    "                    self.update_weights(encoder_grads, decoder_grads)\n",
    "                    # print(f\"Epoch {epoch+1}, Batch {i//self.batch_size+1}, Loss: {loss}\") \n",
    "            elif self.optimizer == 'full_batch':\n",
    "                latent_Z, latent_activations, reconstructed_Z, reconstructed_activations = self.forward(X)\n",
    "                encoder_grads, decoder_grads, loss = self.backward(X, latent_Z, latent_activations, reconstructed_Z, reconstructed_activations)\n",
    "                self.update_weights(encoder_grads, decoder_grads)\n",
    "\n",
    "            # Compute validation loss\n",
    "            if X_val is not None:\n",
    "                _, _, _, decoded_val = self.forward(X_val)\n",
    "                val_loss = self._compute_loss(X_val, decoded_val[-1])\n",
    "            \n",
    "            if self.wandb_vaar:\n",
    "                log_dict = {\n",
    "                    \"epoch\": epoch+1,\n",
    "                    \"train/loss\": loss,\n",
    "                }\n",
    "                if X_val is not None:\n",
    "                    log_dict[\"val/loss\"] = val_loss\n",
    "                wandb.log(log_dict)\n",
    "                \n",
    "            print(f\"{epoch} epochs\")\n",
    "\n",
    "        if self.wandb_vaar and self.wandb_finish:\n",
    "            wandb.finish()\n",
    "\n",
    "    def _loss(self):\n",
    "        if self.loss_function == 'mse':\n",
    "            return self.encoder.mse, self.encoder.mse_derivative\n",
    "        else:\n",
    "            raise ValueError('Loss function not supported')\n",
    "\n",
    "    def _compute_loss(self, X, decoded):\n",
    "        loss, _ = self._loss()\n",
    "        return loss(X, decoded)\n",
    "\n",
    "    def encode(self, X):\n",
    "        return self.encoder.predict(X)\n",
    "\n",
    "    def decode(self, Z):\n",
    "        return self.decoder.predict(Z)\n",
    "\n",
    "    def reconstruct(self, X):\n",
    "        encoded = self.encode(X)\n",
    "        return self.decode(encoded)\n",
    "\n",
    "    def get_latent(self, X):\n",
    "        return self.encoder.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder:\n",
    "    def __init__(self, input_size, hidden_layers, latent_dim, epochs=100, learning_rate=0.01, \n",
    "                 batch_size=32, activation_function='relu', loss_function='mse', \n",
    "                 optimizer='sgd', beta=0.9, Random_state=None, weight_init='he', \n",
    "                 wandb_vaar=False, run_start=\"autoencoder_training\", wandb_finish=True):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.latent_dim = latent_dim\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.beta = beta\n",
    "        self.weight_init = weight_init\n",
    "        self.wandb_vaar = wandb_vaar\n",
    "        self.wandb_finish = wandb_finish\n",
    "        \n",
    "        full_layers = hidden_layers + [latent_dim] + hidden_layers[::-1]    \n",
    "\n",
    "        self.mlp = MLP(input_size=input_size, hidden_layers=full_layers, output_size=input_size,\n",
    "                       epochs=epochs, learning_rate=learning_rate, batch_size=batch_size,\n",
    "                       activation_function=activation_function, loss_function=loss_function,\n",
    "                       optimizer=optimizer, beta=beta, Random_state=Random_state,\n",
    "                       weight_init=weight_init, model_type='regression', wandb_vaar=wandb_vaar,\n",
    "                       run_start=run_start, wandb_finish=wandb_finish)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, X_val=None):\n",
    "        self.mlp.fit(X, X)\n",
    "        \n",
    "    def encode(self, X):\n",
    "        weights = self.mlp.weights[:len(self.hidden_layers)+1]\n",
    "        biases = self.mlp.biases[:len(self.hidden_layers)+1]\n",
    "        activations = [X]\n",
    "        \n",
    "        # do forward pass\n",
    "        activation, _ = self.mlp._activation(self.activation_function)\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            z = np.dot(activations[-1], weights[i]) + biases[i]\n",
    "            activations.append(activation(z))\n",
    "        \n",
    "        return activations[-1]\n",
    "    \n",
    "    def reconstruct(self, X):\n",
    "        weights = self.mlp.weights[len(self.hidden_layers)+1:]\n",
    "        biases = self.mlp.biases[len(self.hidden_layers)+1:]\n",
    "        activations = [X]\n",
    "        \n",
    "        # do forward pass\n",
    "        activation, _ = self.mlp._activation(self.activation_function)\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            z = np.dot(activations[-1], weights[i]) + biases[i]\n",
    "            activations.append(activation(z))\n",
    "        \n",
    "        return activations[-1]\n",
    "        \n",
    "        \n",
    "\n",
    "# test the MLP class\n",
    "auto = AutoEncoder(32, [24, 12], 6, epochs=100, learning_rate=0.01, batch_size=32, activation_function='relu', loss_function='mse', optimizer='mini_batch', beta=0.9, Random_state=None, weight_init='he', wandb_vaar=False, run_start=\"autoencoder_training\", wandb_finish=False)\n",
    "\n",
    "X = np.random.randn(10000, 32)\n",
    "auto.fit(X)\n",
    "\n",
    "X_latent = auto.encode(X)\n",
    "\n",
    "print(X_latent.shape)\n",
    "\n",
    "# reconstruct the input\n",
    "X_reconstructed = auto.reconstruct(X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "(90380, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "modulepath = os.path.abspath(os.path.join('../../models/knn'))\n",
    "sys.path.append(modulepath)\n",
    "modulepath = os.path.abspath(os.path.join('../../performance_measures'))\n",
    "sys.path.append(modulepath)\n",
    "modulepath = os.path.abspath(os.path.join('../../models/linear_regression'))\n",
    "sys.path.append(modulepath)\n",
    "\n",
    "# Load and process the data\n",
    "df = pd.read_csv('../../data/external/spotify-2/train.csv')\n",
    "\n",
    "# extract all the numerical columns\n",
    "n_cols = ['popularity','duration_ms', 'explicit', 'danceability', 'energy', 'key', 'loudness',\n",
    "    'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n",
    "    'valence', 'tempo', 'time_signature', 'track_genre']\n",
    "\n",
    "# print('Number of useful cols :',len(n_cols))\n",
    "\n",
    "# load the n_cols\n",
    "df_cols = df[n_cols]\n",
    "\n",
    "# Normalise the data remove the track_genre while normalising\n",
    "df_cols = df_cols.drop(columns=['track_genre'])\n",
    "df_cols = (df_cols - df_cols.mean())/df_cols.std()\n",
    "df_cols['track_genre'] = df['track_genre']\n",
    "\n",
    "# encode track_genre\n",
    "unique_genres = df_cols['track_genre'].unique()\n",
    "dict_genres = {unique_genres[i]:i for i in range(len(unique_genres))}\n",
    "# print(dict_genres)\n",
    "\n",
    "# density plot using sns\n",
    "# print(df_cols.describe().T)\n",
    "\n",
    "# shuffle the data\n",
    "df_cols_train = df_cols.sample(frac=1)\n",
    "\n",
    "# Load and process the data\n",
    "df = pd.read_csv('../../data/external/spotify-2/test.csv')\n",
    "\n",
    "# extract all the numerical columns\n",
    "n_cols = ['popularity','duration_ms', 'explicit', 'danceability', 'energy', 'key', 'loudness',\n",
    "    'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n",
    "    'valence', 'tempo', 'time_signature', 'track_genre']\n",
    "\n",
    "# print('Number of useful cols :',len(n_cols))\n",
    "\n",
    "# load the n_cols\n",
    "df_cols = df[n_cols]\n",
    "\n",
    "# Normalise the data remove the track_genre while normalising\n",
    "df_cols = df_cols.drop(columns=['track_genre'])\n",
    "df_cols = (df_cols - df_cols.mean())/df_cols.std()\n",
    "df_cols['track_genre'] = df['track_genre']\n",
    "\n",
    "# encode track_genre\n",
    "unique_genres = df_cols['track_genre'].unique()\n",
    "dict_genres = {unique_genres[i]:i for i in range(len(unique_genres))}\n",
    "# print(dict_genres)\n",
    "\n",
    "# density plot using sns\n",
    "# print(df_cols.describe().T)\n",
    "\n",
    "# shuffle the data\n",
    "df_cols_test = df_cols.sample(frac=1)\n",
    "\n",
    "# Load and process the data\n",
    "df = pd.read_csv('../../data/external/spotify-2/validate.csv')\n",
    "\n",
    "# extract all the numerical columns\n",
    "n_cols = ['popularity','duration_ms', 'explicit', 'danceability', 'energy', 'key', 'loudness',\n",
    "    'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n",
    "    'valence', 'tempo', 'time_signature', 'track_genre']\n",
    "\n",
    "# print('Number of useful cols :',len(n_cols))\n",
    "\n",
    "# load the n_cols\n",
    "df_cols = df[n_cols]\n",
    "\n",
    "# Normalise the data remove the track_genre while normalising\n",
    "df_cols = df_cols.drop(columns=['track_genre'])\n",
    "df_cols = (df_cols - df_cols.mean())/df_cols.std()\n",
    "df_cols['track_genre'] = df['track_genre']\n",
    "\n",
    "# encode track_genre\n",
    "unique_genres = df_cols['track_genre'].unique()\n",
    "dict_genres = {unique_genres[i]:i for i in range(len(unique_genres))}\n",
    "# print(dict_genres)\n",
    "\n",
    "# density plot using sns\n",
    "# print(df_cols.describe().T)\n",
    "\n",
    "# shuffle the data\n",
    "df_cols_val = df_cols.sample(frac=1)\n",
    "\n",
    "# Split the data into train test val 80 10 10\n",
    "# train_size = int(0.8 * len(df_cols))\n",
    "# validate_size = int(0.1 * len(df_cols))\n",
    "# test_size = len(df_cols) - train_size - validate_size\n",
    "\n",
    "X_train = df_cols_train\n",
    "X_validate = df_cols_val\n",
    "X_test = df_cols_test\n",
    "\n",
    "y_train = X_train.pop('track_genre')\n",
    "y_validate = X_validate.pop('track_genre')\n",
    "y_test = X_test.pop('track_genre')\n",
    "\n",
    "# final check\n",
    "X_train = X_train.values\n",
    "X_validate = X_validate.values\n",
    "X_test = X_test.values\n",
    "\n",
    "# encode y using dict_genres\n",
    "y_train = y_train.map(dict_genres).values\n",
    "y_validate = y_validate.map(dict_genres).values\n",
    "y_test = y_test.map(dict_genres).values\n",
    "\n",
    "# # use the autoencoder to get the latent representation of the data\n",
    "# autoencoder = Autoencoder(input_size=X_train.shape[1], hidden_layers=[64,32,16], latent_dim=6, epochs=100, learning_rate=0.1, \n",
    "#                           batch_size=32, activation_function='relu', loss_function='mse', optimizer='mini_batch', beta=0.9, \n",
    "#                           Random_state=None, weight_init='he', wandb_vaar=True, run_start=\"autoencoder_training\", wandb_finish=False)\n",
    "\n",
    "# autoencoder.fit(X_train, X_validate)\n",
    "\n",
    "# use autoEncoder\n",
    "auto = AutoEncoder(input_size=X_train.shape[1], hidden_layers=[32,12], latent_dim=6, epochs=100, learning_rate=0.4,\n",
    "                   batch_size=32, activation_function='sigmoid', loss_function='mse', optimizer='mini_batch', beta=0.9,\n",
    "                   Random_state=None, weight_init='he', wandb_vaar=False, run_start=\"autoencoder_training\", wandb_finish=True)\n",
    "\n",
    "auto.fit(X_train)\n",
    "\n",
    "X_train_latent = auto.encode(X_train)\n",
    "X_validate_latent = auto.encode(X_validate)\n",
    "X_test_latent = auto.encode(X_test)\n",
    "\n",
    "print(X_train_latent.shape)\n",
    "\n",
    "# # Get the latent representation of the data\n",
    "# X_train_latent = autoencoder.get_latent(X_train)\n",
    "# X_validate_latent = autoencoder.get_latent(X_validate)\n",
    "# X_test_latent = autoencoder.get_latent(X_test)\n",
    "\n",
    "\n",
    "# # print('Train size:', X_train.shape, y_train.shape)\n",
    "# # print(y_train[:10])\n",
    "\n",
    "# # knn classifier\n",
    "# from knn import KNNClassifier_P\n",
    "\n",
    "# knn = KNNClassifier_P(30, distance_metric='manhattan')\n",
    "# knn.fit(X_train, y_train)\n",
    "# start_time = time.time()\n",
    "# y_pred = knn.predict(X_test)\n",
    "# end_time = time.time()\n",
    "\n",
    "# from metrics import Metrics\n",
    "\n",
    "# metrics = Metrics(y_test, y_pred)\n",
    "\n",
    "# metrics.print_report()\n",
    "\n",
    "# print(\"Accuracy:\", np.mean(y_test == y_pred))\n",
    "# print(\"Time:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction error: 0.7373066532758051\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find reconstruction error\n",
    "X_reconstructed = auto.reconstruct(X_train_latent)\n",
    "reconstruction_error = np.mean((X_train - X_reconstructed)**2)\n",
    "print(\"Reconstruction error:\", reconstruction_error)\n",
    "\n",
    "# plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "loss = auto.mlp.loss\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (90380, 6) (11400, 6)\n",
      "Accuracy: 0.1254\n",
      "Macro Precision: 0.1105\n",
      "micro Precision: 0.1254\n",
      "Macro Recall: 0.1266\n",
      "Micro Recall: 0.1254\n",
      "Macro F1 Score: 0.1094\n",
      "Micro F1 Score: 0.1254\n",
      "Specificity: 0.9923\n",
      "Accuracy: 0.12535087719298246\n",
      "Time: 22.79715061187744\n"
     ]
    }
   ],
   "source": [
    "# Get the latent representation of the data\n",
    "\n",
    "\n",
    "print('Train size:', X_train_latent.shape, X_test_latent.shape)\n",
    "# print(y_train[:10])\n",
    "\n",
    "# knn classifier\n",
    "from knn import KNNClassifier_P\n",
    "\n",
    "knn = KNNClassifier_P(30, distance_metric='manhattan')\n",
    "knn.fit(X_train_latent, y_train)\n",
    "start_time = time.time()\n",
    "y_pred = knn.predict(X_test_latent)\n",
    "end_time = time.time()\n",
    "\n",
    "from metrics import Metrics\n",
    "\n",
    "metrics = Metrics(y_test, y_pred)\n",
    "\n",
    "metrics.print_report()\n",
    "\n",
    "print(\"Accuracy:\", np.mean(y_test == y_pred))\n",
    "print(\"Time:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90380, 15) (90380,)\n",
      "Accuracy: 0.2491\n",
      "Macro Precision: 0.2383\n",
      "micro Precision: 0.2491\n",
      "Macro Recall: 0.2505\n",
      "Micro Recall: 0.2491\n",
      "Macro F1 Score: 0.2322\n",
      "Micro F1 Score: 0.2491\n",
      "Specificity: 0.9934\n",
      "Accuracy: 0.24912280701754386\n",
      "Time: 31.092090368270874\n"
     ]
    }
   ],
   "source": [
    "# knn classifier\n",
    "from knn import KNNClassifier_P\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "knn = KNNClassifier_P(30, distance_metric='manhattan')\n",
    "knn.fit(X_train, y_train)\n",
    "start_time = time.time()\n",
    "y_pred = knn.predict(X_test)\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "from metrics import Metrics\n",
    "\n",
    "metrics = Metrics(y_test, y_pred)\n",
    "\n",
    "metrics.print_report()\n",
    "\n",
    "print(\"Accuracy:\", np.mean(y_test == y_pred))\n",
    "print(\"Time:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113]\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n"
     ]
    }
   ],
   "source": [
    "print((np.unique(y_train)))\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_validate = y_validate.reshape(-1,1)\n",
    "\n",
    "# train the mlp model\n",
    "mlp = MLP(input_size=X_train.shape[1], hidden_layers=[24,48], output_size=len(np.unique(y_train))+1, epochs=100, learning_rate=0.1, \n",
    "          batch_size=32, activation_function='sigmoid', loss_function='cross_entropy', optimizer='mini_batch', beta=0.9, Random_state=None, \n",
    "          weight_init='he', model_type='class_MC', wandb_vaar=False, run_start=\"autoencoder_training\", wandb_finish=False)\n",
    "\n",
    "# train the model\n",
    "mlp.fit(X_train, y_train, X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11400, 114)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "print(y_pred.shape)\n",
    "\n",
    "# plot loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(mlp.loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 accuracy: 0.08578947368421053\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113]\n",
      "Accuracy: 0.0942\n",
      "Macro Precision: 0.9825\n",
      "micro Precision: 0.0942\n",
      "Macro Recall: 0.0936\n",
      "Micro Recall: 0.0942\n",
      "Macro F1 Score: 0.1524\n",
      "Micro F1 Score: 0.0942\n",
      "Specificity: 0.9920\n"
     ]
    }
   ],
   "source": [
    "y_pred_solved = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# save top 5 predictions\n",
    "y_pred_top5 = np.argsort(y_pred, axis=1)[:,-10:]\n",
    "\n",
    "# if the true label is in the top 5 predictions\n",
    "top5 = np.zeros_like(y_test)\n",
    "new_y_pred = np.zeros_like(y_test)\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] in y_pred_top5[i]:\n",
    "        top5[i] = 1\n",
    "        new_y_pred[i] = y_test[i]\n",
    "\n",
    "print(\"Top 5 accuracy:\", np.mean(top5))\n",
    "\n",
    "print(np.unique(y_pred_solved))\n",
    "\n",
    "metrics = Metrics(y_test, new_y_pred)\n",
    "\n",
    "metrics.print_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
